<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Adam Li's blog - Academic</title><link href="/" rel="alternate"></link><link href="/feeds/academic.atom.xml" rel="self"></link><id>/</id><updated>2021-02-02T00:00:00-05:00</updated><entry><title>Writing an Academic Journal Paper</title><link href="/blog/2021/02/writing-journal-papers/" rel="alternate"></link><published>2021-02-02T00:00:00-05:00</published><updated>2021-02-02T00:00:00-05:00</updated><author><name>Adam Li</name></author><id>tag:None,2021-02-02:/blog/2021/02/writing-journal-papers/</id><summary type="html">&lt;p&gt;A short walkthrough of my experience of writing an academic journal paper.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Writing Academic Journal Papers&lt;/h1&gt;
&lt;p&gt;Academic journal papers (preprint and peer-reviewed published) are the lifeblood of scientific researchers. These
samples of writing describe pieces of scientific work in a certain sub-field. For example, it might describe experiments
demonstrating the mechanisms of action to generate an action potential, or a cohort study of epilepsy patients that
underwent surgery or drug treatment. These are generally characterized by hypotheses, prior art motivation and a
rigorous methodology that leads to falsifiable results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Here, I assume that you have generated all your results already! I mainly discuss optimal workflows for writing papers
and dealing with revisions.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Recommended Tools For Saving Time and Preventing Headaches&lt;/h2&gt;
&lt;p&gt;These are recommended tools for writing a paper.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://github.com/"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://overleaf.com/"&gt;Overleaf for Latex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gigantum.com/"&gt;Gigantum for web-based reproducing computational experiments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ctan.org/pkg/latexdiff?lang=en"&gt;latexdiff for generating diff between two latex files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://inkscape.org/"&gt;inkscape (or Adobe Photoshop if you can afford it)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mendeley.com/download-desktop-new/"&gt;Mendeley or Zotero&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Preprint-servers: https://www.biorxiv.org/submit-a-manuscript and https://www.arxiv.org&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openneuro.org/"&gt;open-neuro for depositing open-access data&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Really, overleaf has an awesome interface that lets you look at latex and PDF and table of contents 
simultaneously.&lt;/p&gt;
&lt;p&gt;&lt;img alt="overleaf example" src="../../images/blog/overleaf_example.png"&gt;&lt;/p&gt;
&lt;p&gt;The tools are recommended for the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Version-control&lt;/strong&gt;: whether it is code, manuscript, references, or data, version control is important!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ease-of-collaboration&lt;/strong&gt;: Many times you are going to be writing a paper simultaneously with collaborators, you would 
   to be able to work on things simultaneously! In addition, you could ideally comment in-place (yay overleaf).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modularity&lt;/strong&gt;: if you have ever used Microsoft word and tried to place a figure/caption near text, it is a nightmare. 
   If you have ever written 5 sections of a manuscript, and then had your PI want to take out 2-3 and then put back 1-2 
   at a later date, you'll wish you had some way of turning on/off sections. Latex is by design modular.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt;: Whether it is data analyses, or actually plotting the figures, science relies on these items being 
   fully reproducible by a 3rd party. Many of the steps that lead to these final presentations requires a lot of 
   package installations, environment handling and more. Gigantum is a nice mechanism for having a reproducible engine 
   hosted on the web. Github can maintain the original versions of your code used to produce said analyses/figures.&lt;/li&gt;
&lt;li&gt;Math typing: typing math in microsoft word is a painful experience&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Some possible reasons for not using the above tools&lt;/h3&gt;
&lt;p&gt;Note that many of these tools require some bit of a learning curve. Notably, overleaf for latex, and Mendeley/Zotero 
for references. If you have collaborators that simply cannot use these tools, then that's an unfortunate 
circumstance... I have yet to discover an optimal workflow besides: render pdf / convert to word using Adobe, and 
then incorporate changes/notes afterwards.&lt;/p&gt;
&lt;p&gt;Here's a good comparison on &lt;a href="https://openwetware.org/wiki/Word_vs._LaTeX#:~:text=Comparison%20of%20Word%20and%20Latex&amp;amp;text=The%20strength%20of%20Word%20is,layout%20in%20a%20separate%20step."&gt;word vs latex&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Other notable tools&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Google docs&lt;/li&gt;
&lt;li&gt;&lt;a href="https://workspace.google.com/marketplace/app/autolatex_equations/850293439076?pann=cwsdp&amp;amp;hl=en-US"&gt;auto-latex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://paperpile.com/"&gt;paperpile&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you intend on using these tools, possibly a similar workflow applies, but these are not directly discussed.&lt;/p&gt;
&lt;h2&gt;Starting a Paper (Latex)&lt;/h2&gt;
&lt;p&gt;I recommend using Latex to write papers because they are modular and support version-control, whereas Word documents
inherently lack these features. In order to use latex, one can setup a local environment, but this becomes difficult 
to share. Therefore, we highly recommend Overleaf, which is an online latex editor that allows for 
tracking changes and simultaneous editing across many people. In addition, if you pay for the subscription, it 
allows integration with Github/Dropbox/Mendeley/Zotero.&lt;/p&gt;
&lt;h2&gt;Reproducible Figures and Main Experiments (Computational)&lt;/h2&gt;
&lt;p&gt;Many journal papers are moving towards reproducability, since this has become a major 
issue in recent years. If you setup your code to do so, it helps facilitate this process, 
but also has the added benefit that &lt;code&gt;people will more likely cite your study&lt;/code&gt;, since they 
can confirm results for themselves! In order to facilitate &lt;strong&gt;trivial&lt;/strong&gt; reproduction of your 
main figures in the paper, I suggest using a github repository with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;README&lt;/code&gt; file describing installation process, main instructions to generate figures, instructions for downloading
  additional data and any misc. notes that users need to be aware of.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; directory containing all the source data needed to reproduce figures and or run the experiment. These should
  be relatively small files, since they are housed in a Github repository.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;*.ipynb&lt;/code&gt; Jupyter notebooks that contain self-contained code to generate all main 
  figures and/or experiments described in the paper.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One should also include scripts (e.g. &lt;code&gt;.py&lt;/code&gt; files) needed to support the generation 
of figures and/or experiments as well. &lt;/p&gt;
&lt;h2&gt;Modularizing and Creating Figures&lt;/h2&gt;
&lt;p&gt;Many journal papers before publishing your manuscript will want the raw high-resolution figures. 
This generally means using &lt;code&gt;.pdf&lt;/code&gt;, or &lt;code&gt;.svg&lt;/code&gt; format, so that they do not lose resolution during
zooming in/out operations. In order to prepare for these later on, it is helpful to have each 
figure/subfigure in a file already! Inkscape (and/or Photoshop) can load SVG/PDF files and 
modify them. If you edit figures in these programs, then if you need to perform downstream 
edits (due to revisions), it becomes easy because you just pull up the file and perform edits.&lt;/p&gt;
&lt;p&gt;Due to latex modularizing figures, you simply have to re-save the figures with the same filepath
as where your latex file is pulling from. An example with Overleaf and Dropbox integration would be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You have Dropbox integrating your Overleaf document locally, and you need to change Figure 1, 
   stored at &lt;code&gt;figures/figure-1-blah.svg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Use inkscape to modify the figure and re-save at &lt;code&gt;figures/figure-1-blah.svg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Allow Dropbox/Overleaf to integrate (usually takes a few seconds).&lt;/li&gt;
&lt;li&gt;Re-render the latex/PDF of the manuscript.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Modularizing References and Citations&lt;/h2&gt;
&lt;p&gt;When writing a manuscript, you'll need to keep track of all your references! Managing this manually 
is unwieldy and extremely prone to error. Use Mendeley (and/or Zotero)! These tools all have 
Overleaf integration, Desktop application and a Chrome browser extension. Thus one's workflow looks like:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find reference online&lt;/li&gt;
&lt;li&gt;Use browser extension to add reference and metadata to a library group&lt;/li&gt;
&lt;li&gt;Export bibliography (&lt;code&gt;.bib&lt;/code&gt;) file to directory for your Latex file to use&lt;/li&gt;
&lt;li&gt;Re-render Latex PDF file&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This has the advantage that one can easily change the citation formats, edit metadata and easily 
port the bibliography file to other manuscripts. &lt;/p&gt;
&lt;h2&gt;Dealing with Revisions&lt;/h2&gt;
&lt;p&gt;When dealing with revisions, one generally will need to generate a tracked-diff between the 
submission file and the revision file. This helps facilitate editor/reviewer feedback and 
if you do not do this, you will not be liked! This is obviously trivial with Microsoft Word, 
but it is an incredible headache to convert Latex/PDF files to Word. &lt;/p&gt;
&lt;p&gt;To handle a tracked-diff, &lt;code&gt;save a copy of the Latex version upon initial manuscript submission.&lt;/code&gt;
Then, when you have your final revised manuscript after revision, use &lt;code&gt;latexdiff&lt;/code&gt; to generate 
a new Latex file, which will now render a nice PDF with tracked changes inside the PDF file itself!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;latexdiff-so old_version.tex new_version.tex &amp;gt; differences.tex&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here's a great reference: http://www.peteryu.ca/tutorials/publishing/latexdiff&lt;/p&gt;
&lt;h2&gt;(Optional) Converting Latex/PDF to Word&lt;/h2&gt;
&lt;p&gt;The only tool I know of currently that even relatively works is Adobe's export PDF to word 
functionality. Even there, it does not work great when you have mathematical equations,&lt;br&gt;
complex figures, and custom formatting! However, some journals for some reason still require
a word document for initial submission, so there you have it.&lt;/p&gt;
&lt;h1&gt;Submitting to Preprint&lt;/h1&gt;
&lt;p&gt;Great! Now you have your manuscript version 1 ready to go. Well, unfortunately the submission process for many 
academic journals is more involved. Fortunately, you can still make your work publicly accessible via preprint 
servers! &lt;/p&gt;
&lt;p&gt;The preprint servers (&lt;a href="https://submit.biorxiv.org/"&gt;bioarxiv&lt;/a&gt; and &lt;a href="https://arxiv.org/user/"&gt;arxiv&lt;/a&gt;) are extremely 
easy to release a preprint. Bioarxiv is for generally more biological related sciences and arxiv is for the 
physical and mathematical sciences. There are also other preprint servers, like Medarxiv.&lt;/p&gt;
&lt;p&gt;One must have:
- the rendered PDF files (main manuscript +/- supplemental pdf)
- supplemental data (optional)
- figures data (optional)
- emails and full names of all collaborators&lt;/p&gt;
&lt;p&gt;and you are ready to go! Just make an account with the preprint server you are using and go through 
their submission process!&lt;/p&gt;
&lt;h2&gt;What to do next?&lt;/h2&gt;
&lt;p&gt;Nowadays, there is a deluge of preprints. To "advertise" your study, it is 
recommended to send this out to 1) collaborators and 2) post on Twitter!&lt;/p&gt;
&lt;p&gt;Last but not least, remember that preprints are version controlled! You 
can always submit a new version when work has been improved and updated.&lt;/p&gt;</content><category term="Academic"></category><category term="writing"></category><category term="phd"></category><category term="johns hopkins"></category></entry><entry><title>Real Analysis (Lebesgue Integration, Differentiation and Measure)</title><link href="/blog/2020/12/real-analysis/" rel="alternate"></link><published>2020-12-24T00:00:00-05:00</published><updated>2020-12-24T00:00:00-05:00</updated><author><name>Adam Li</name></author><id>tag:None,2020-12-24:/blog/2020/12/real-analysis/</id><summary type="html">&lt;p&gt;A short summary of important concepts in Real Analysis&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Real analysis is the study of real numbers in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt;, with 
&lt;code&gt;d&lt;/code&gt; being the dimensionality. Although it is a pretty abstract topic 
that does not seem to have applications, it is a very fundamental 
in understanding other branches of applied mathematics. For example, in order
to define optimization algorithms, one is very interested in the ideas of convergence.
One then often uses theory of continuity. In computational neuroscience and 
control theory, one often deals with integrals when defining theory. In 
statistics, it is absolutely necessary to understand integrals (expectation 
is defined with an integral).&lt;/p&gt;
&lt;p&gt;In this blog post, I'll review some of the basic concepts that one
needs to know in general. A great course sequence I took was both at UCSD with 
Professor Dragos Oprea (Math 140AB) and with Professor Sogge (Math 605) at Johns Hopkins.&lt;/p&gt;
&lt;p&gt;Excellent references are:
- &lt;a href="http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf"&gt;Stein, E. M., Stein, E. M., &amp;amp; Shakarchi, R. (2005). Princeton lectures in analysis: 3. Princeton: Princeton Univ. Press.&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Measurability&lt;/h1&gt;
&lt;p&gt;Measure is an abstract notion of "volume" of a set. In all "regular" 
examples, one can assume the sets in consideration are measurable. However,
using the &lt;a href="https://en.wikipedia.org/wiki/Axiom_of_choice"&gt;Axiom of Choice&lt;/a&gt;, one
can construct "unmeasurable" sets. This motivates the need to define Borel sets (sigma-algebras)
in probability theory, which denote measurable family of subsets of a set. This 
is beyond the scope of this post though. It suffices to know that unmeasurable 
sets &lt;code&gt;do exist&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Measure from first principles is defined in Stein starting from outer-measure. 
Outer-measure is simply the measure obtained by covering a set with a countable 
almost-disjoint union of rectangles (circles, squares also suffice). Outer-measure
is well-defined for any possible set. Lebesgue measure then is defined by 
the infimum over the possible outer-measures of a set.&lt;/p&gt;
&lt;p&gt;Some important properties of measure of a set is non-negativity and 
countable sub-additivity. Countable additivity is obtained when 
they are pairwise disjoint sets in consideration.&lt;/p&gt;
&lt;h1&gt;Convergence&lt;/h1&gt;
&lt;p&gt;For many interesting functions, and numbers, we are interested in 
the notion of convergence. That is, if we define a sequence enumerated 
by &lt;code&gt;n&lt;/code&gt; of numbers, or functions using some rule that depends on &lt;code&gt;n&lt;/code&gt;, 
then what are conditions that these sequences converge? Generally, 
if a sequence does not converge, then it goes to &lt;span class="math"&gt;\(\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In real analysis, we generally consider the Euclidean metric space. 
In fact, these metric spaces are complete (i.e. all Cauchy sequences converge
inside the metric space).&lt;/p&gt;
&lt;h2&gt;Important Integral Convergence Theorems&lt;/h2&gt;
&lt;p&gt;When defining convergence for integrals, there are three fundamental results 
that are useful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.math3ma.com/blog/fatous-lemma"&gt;Fatou's lemma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.math3ma.com/blog/monotone-convergence-theorem"&gt;Monotone convergence theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.math3ma.com/blog/dominated-convergence-theorem"&gt;Dominated convergence theorem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I won't talk about them here because the above links actually provide 
a very nice reference (that is also accessible to beginners).&lt;/p&gt;
&lt;h2&gt;Bolzano-Weirstrass Theorem For Sequences of Real Numbers&lt;/h2&gt;
&lt;p&gt;The main way of proving convergence relies on showing that a sequence 
is Cauchy. That is for every &lt;span class="math"&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;, we have:&lt;/p&gt;
&lt;div class="math"&gt;$$|x_m - x_n| &amp;lt; \epsilon$$&lt;/div&gt;
&lt;p&gt;for &lt;span class="math"&gt;\(m, n \ge N\)&lt;/span&gt;, for some large number &lt;span class="math"&gt;\(N_\epsilon\)&lt;/span&gt;. The "|...|" sign 
denotes the distance metric defined. This implies that even if 
the sequence &lt;span class="math"&gt;\(\{x_k\}\)&lt;/span&gt; potentially oscillates, the range of its oscillation
is decreasing as you take values further and further down the sequence.
Note that by construction any sequence that is Cauchy in a metric space 
is in fact &lt;code&gt;bounded&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Bolzano%E2%80%93Weierstrass_theorem"&gt;Bolzano-Weirstrass theorem, or sequential compactness theorem&lt;/a&gt; 
states that any bounded sequence in finite-dimensional Euclidean space has 
a convergent subsequence. Then a sequence is convergent if and only if 
it has a convergent subsequence, and so every Cauchy sequence is convergent.&lt;/p&gt;
&lt;h2&gt;Convergence In Norm&lt;/h2&gt;
&lt;p&gt;Note that convergence of a sequence of real numbers requires a specific 
component to emerge. In the space of integrable functions, &lt;span class="math"&gt;\(L^1(\mathbb{R}^d)\)&lt;/span&gt;, 
one can define &lt;code&gt;convergence in norm&lt;/code&gt;. That is the sequence defined might not 
converge exactly element by element to a fixed component, but their difference in norm
converges. That is for every &lt;span class="math"&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\{f_k\} \in L^1\)&lt;/span&gt;, we have:&lt;/p&gt;
&lt;div class="math"&gt;$$||f_m(x) - f_n(x)||_{L^1} &amp;lt; \epsilon$$&lt;/div&gt;
&lt;p&gt;where the distance metric is now defined as &lt;span class="math"&gt;\(||.||_{L^1} = \int_{\mathbb{R}^d} |f_m(x) - f_n(x)| dx\)&lt;/span&gt;. 
Convergence in norm can then be extended in general to &lt;span class="math"&gt;\(L^p\)&lt;/span&gt; spaces, since 
&lt;span class="math"&gt;\(L^1 \supset L^2 \supset ... \supset L^\infty\)&lt;/span&gt;, which follows from Holder's 
inequality.&lt;/p&gt;
&lt;h2&gt;Other Convergences&lt;/h2&gt;
&lt;p&gt;Note that there are other notions of convergences as well! 
Weak convergence, strong convergence, etc.&lt;/p&gt;
&lt;h1&gt;Integrability&lt;/h1&gt;
&lt;p&gt;Integration with respect to Lebesgue measure is defined by considering the 
convergence of the integral of the absolute value of the function:&lt;/p&gt;
&lt;div class="math"&gt;$$\int |f(x)| dx &amp;lt; \infty$$&lt;/div&gt;
&lt;p&gt;else, the function f is said to be not integrable. The space of 
Lebesgue integrable functions if formally known as the "L1" space. 
Note that by the Risz-Fisher theorem, we know that &lt;span class="math"&gt;\(L^p\)&lt;/span&gt; spaces for 
&lt;span class="math"&gt;\(1 \le p \le \infty\)&lt;/span&gt; is complete. &lt;/p&gt;
&lt;h1&gt;Differentiation&lt;/h1&gt;
&lt;p&gt;When considering differentiation in the context of integration, 
we are primarily interested in when the Fundamental Theorems of Calculus hold
in the Lebesgue measure theory. We learned in high school calculus the following:&lt;/p&gt;
&lt;h2&gt;First Fundamental Theorem of Calculus:&lt;/h2&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(F(x) = \int_a^x f(t) dt\)&lt;/span&gt; for &lt;span class="math"&gt;\(x \in [a, b]\)&lt;/span&gt; interval, then&lt;/p&gt;
&lt;div class="math"&gt;$$F'(x) = f(x)$$&lt;/div&gt;
&lt;p&gt;that is, we can reverse integration on f by taking the derivative of F.
This theorem holds in the space of Lebesgue integrable functions, when the 
function is &lt;code&gt;locally integrable&lt;/code&gt; (i.e. Lebesgue's differentiation theorem).&lt;/p&gt;
&lt;h2&gt;Second Fundamental Theorem of Calculus&lt;/h2&gt;
&lt;p&gt;The second part states that if we take the endpoints of the antiderivative, 
then we can obtain the integral of the function over those endpoints:&lt;/p&gt;
&lt;div class="math"&gt;$$F(b) - F(a) = \int_a^b f(t) dt$$&lt;/div&gt;
&lt;p&gt;The result holds when F is &lt;code&gt;absolutely continuous&lt;/code&gt;, which is a stronger 
notion then continuity. It essentially bounds the degree of variation and is 
actually introduced through the concept of Bounded Variation.&lt;/p&gt;
&lt;h1&gt;Other Important Topics&lt;/h1&gt;
&lt;p&gt;This blog post is meant to be brief, but in the future, I'll probably 
try to cover Hilbert spaces (linear operators, orthogonality, compact operators, 
the Spectral Theorem, and Kernel integral operators), Fourier series and the Fourier transform, which are also 
broadly applicable across applied mathematics and engineering.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Academic"></category><category term="math"></category><category term="phd"></category><category term="real-analysis"></category></entry><entry><title>Using The Virtual Brain (TVB) to Understand Algorithms</title><link href="/blog/2018/06/whitaker-summary-experience/" rel="alternate"></link><published>2018-06-01T00:00:00-04:00</published><updated>2018-06-01T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2018-06-01:/blog/2018/06/whitaker-summary-experience/</id><summary type="html">&lt;p&gt;To summarize my Whitaker/Chateaubriand research experience abroad in Marseille, France.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Here, I will attempt to give a brief introduction to the core concepts in my research and relate it to the clinical problem of epilepsy.&lt;/p&gt;
&lt;h2&gt;0. Short Blurb About Brains and Network Neuroscience&lt;/h2&gt;
&lt;p&gt;With all the hype about "AI" and deep learning recently, it can 
become easy to assume that we are at the apex of our understanding 
of the human brain. This couldn't be farther from the truth. 
While we have a decent understanding of how single neurons work 
and how different regions of our brain work to give rise to very 
basic behavior (i.e. vision, movement, perception), we essentially 
have no knowledge as to how hundreds-millions of neurons work 
together, or how more complex behavior manifests (i.e. imagination, 
learning, etc.). We don't even know how to quantitatively diagnostic 
epilepsy, Parkinsons disease, Alzheimers disease, or any mental 
disorder for that matter. Brains are inherently a complex network 
of highly nonlinear functions. In the past it has been easy to analyze
very simple, linear "things". That is because linear means a line. 
If you add a lot of lines together, it is easy to predict what the 
result looks like. Now imagine, if instead of a line, you have a 
highly complex curve and you add many of these together. It quickly 
becomes insurmountable to imagine how this result would look like. 
This is exactly what happens with our brain. We initially were able 
to ask simple questions of the brain and get simple answers from one 
neuron, or one region of the brain. However, in order to tackle 
increasingly complex questions of the brain, we have to shift our 
analysis to look at the entire network of the brain. When we learn, 
how does the entire brain coordinate this (not just one region)? 
When we have neurological diseases, how does the entire brain get 
affected (not just one region)?&lt;/p&gt;
&lt;p&gt;Obviously, there is a whole field out there growing as a result. 
That is called, network neuroscience; the science of analyzing 
how the brain works using network-based analysis (e.g. graphs, 
neural networks, etc.). My belief is that as our understanding of 
basic brain network principles improves, our ability to generate 
increasingly complex AI systems will also improve.&lt;/p&gt;
&lt;h2&gt;1. Epilepsy&lt;/h2&gt;
&lt;p&gt;Epilepsy is a disease that affects more than 70 M people worldwide, 
which characterizes itself with seizures (i.e. abnormal brain activity) 
for seconds to several minutes. Epilepsy can be treated with medicine that generally inhibits brain activity (albeit with numerous side effects), and also with surgery. Surgery can involve resection (i.e. cutting a portion of the hypothesized diseased brain) and laser ablation (i.e. heat treatment of small spherical regions within the brain). When successful, surgery can result in complete seizure freedom! &lt;/p&gt;
&lt;p&gt;However, the problem is that surgery is extremely variable in success (e.g. ~50% average). Imagine getting permanent brain surgery, when there is a high likelihood of you still having seizures afterwards. A main obstacle to high success rates is incorrect localization of the epileptogenic zone, the clinical region of the brain that seizures originate from. In addition, clinicians employ a strategy similar to cancer resection; they will cut out a greater portion of the brain then possibly hypothesized because it will have good "margins" on the diseased tissue. This can cause unnecessary neural dysfunction, especially when these regions are close to important areas for language, motor or executive function. The goal of any researcher in this field is to identify biomarkers and methods for robustly identifying this diseased brain region given neural data. The result would be more successful surgeries and more accurate maps, leading to less brain regions being resected. Neural data can come in the form of electrophysiological recordings, MRI images, Diffusion weighted MRI images and CT images.&lt;/p&gt;
&lt;h2&gt;2. Computational Modeling&lt;/h2&gt;
&lt;p&gt;Computational modeling is the art of using mathematical equations to model how certain systems (i.e. the brain) behaves given parameters you input. These are normally formed in the framework of differential equations (i.e. that class we took in undergrad that made us go "huh?"). So here, at Marseille, the group has developed computational models that take in the patient's specific brain imaging to model epilepsy in a patient-specific manner (i.e. personalized brain modeling). The way it does this is by parcellating the brain into multiple regions, where each region is modeled by a computational model that can exhibit certain seizure phenomena. Then, by coupling (connecting) every region based on realistic brain connectivity, you can simulate brain behavior when you "set" different regions of the brain to be "diseased" that has some form of a realistic brain network. One could then compare the simulation and the real electrophysiological recording data in patients to test different hypotheses, such as: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;what happens if we set this region to be epileptogenic? does it resemble the real dynamics recorded in the patient's brain? &lt;/li&gt;
&lt;li&gt;If we remove this region of the brain (i.e. remove connectivity to and from this region), will it help prevent propagation of seizure activity to healthy regions of the brain?&lt;/li&gt;
&lt;li&gt;Can we generate realistic data that can reflect feature variability of the real recording data?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;3. Algorithms&lt;/h2&gt;
&lt;p&gt;Algorithms are spelling out a certain set of computations that a program should undertake to get a specific answer. In our research group, we are attempting to develop algorithms that take in brain data of the patient and outputs a prediction of the diseased region. There are different kinds of algorithms that use the data in various ways. In general, all data analysis tries to represent data in various ways. Does the amplitude, or frequency of the data matter? Should we represent the data as a graph? Should we apply filters to the data to remove noise? Should we just leave the data alone and let the model decide what is important?&lt;/p&gt;
&lt;p&gt;Our research group currently developed a fast network-based algorithm that analyzes the data as a graph by applying a very specific type of computation. Namely, it constructs a graph out of the dataset and applies perturbations to the graph (i.e. add's vectors of noise in a very structured way), to determine which regions of the brain are most susceptible to being perturbed into seizing. This specific type of analysis requires some simple linear systems theory to derive an analytical equation for doing this. It was biologically inspired and can be read in some of the reference publications.&lt;/p&gt;
&lt;p&gt;On the other end of the spectrum, one could apply deep learning to this problem for a way of supervised learning. What this would require is knowing the exact regions of the brain that are diseased and then feeding in the data and the labels of the regions to let the model determine which features of the data are most predictive of the diseased region. This is more general, but can require large amounts of data and hyperparameter tuning of the neural networks. In addition, epilepsy data has the problem of "noisy labeling". Clinicians are never sure where exactly the EZ is, so even if we have a significant amount of patients, our training data for deep learning are not optimal. Contrast this with the infamous case of recognizing "cats vs dogs", where I am pretty positive most cats are definitely correctly labeled as cats, and vice versa.&lt;/p&gt;
&lt;h1&gt;Research Project Concepts&lt;/h1&gt;
&lt;p&gt;Here, I attempt to explain some of the high level concepts that motivated the research I am carrying out that integrates network analysis, computational modeling and deep learning.&lt;/p&gt;
&lt;h2&gt;1. The Virtual Brain (TVB) vs Network Data Analysis&lt;/h2&gt;
&lt;p&gt;This was the main project proposed when I applied for the Whitaker/Chateaubriand fellowships. The goal was to use the flexible modeling capabilities of "The Virtual Brain" (TVB) platform developed here in Marseille to understand how network data predictions of the epileptogenic zone performs under various model configurations. So, how can our predictions work under various clinical settings? Can we arrive at the same conclusion when our algorithm is applied to an in-silico model?&lt;/p&gt;
&lt;p&gt;Being able to demonstrate agreement by using this whole-brain model helps provide evidence on two fronts: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;provides additional evidence that TVB is capable of modeling the dynamical characteristics of seizures realistically and &lt;/li&gt;
&lt;li&gt;provide hypothetical constraints on data analysis by providing ground-truth simulations of epileptic seizures and demonstrating when the algorithms work.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Summary of Work&lt;/strong&gt;
By using a patient's individual neuroimaging scans, and their actual recorded epileptic seizures, we can 1) create a personalized brain from those scans and 2) analyze their electrophysiological recordings with our network-based algorithm. &lt;/p&gt;
&lt;p&gt;From the personalized brain, we can use TVB as a software platform to simulate signals that we hypothesize might come from different regions of that patient's brain. If we fix all parameters, and then systematically change which region of the brain is epileptic, then we can form a suite of datasets where we know the epileptic region(s). This would not be possible with real data because we never know for sure which region of the brain is epileptic! So assuming this model is accurate to some degree in replicating features of the  brain's electrophysiology, we can compare the results of the network-based analysis for each simulated dataset.&lt;/p&gt;
&lt;p&gt;With tools from statistics, we can statistically compare the different results from our algorithm to the real data. The results that are closest to our real data suggest that this is the region most likely to be actually epileptic. This region was where we set as epileptic in the software and was capable of producing the simulated data that best represented the real data; in other words, it is a measure of how similar our simulations are to real data under different hypotheses of epileptic regions. This presents a framework to systematically produce an estimate of the real epileptic region in a patient, and also to study situations in which our network-based algorithm can fail. It is a "step" towards opening the black box, which is the patient's brain.&lt;/p&gt;
&lt;h2&gt;2. Using TVB To Augment Neural Datasets For Deep Learning&lt;/h2&gt;
&lt;p&gt;A core problem of deep learning is the amount of data required to train successful models to perform classification/regression. Generally, more data means more variation in your dataset, thus having a higher likelihood of capturing the true underlying distribution of all possible data. This is especially apparent in models surrounding neuroscience and neural data. That is because neural data is traditionally difficult to obtain, and is extremely constrained because of the different variables that go into collecting the data in a clinical setting. Clinical procedures can vary, recording electrodes can be implanted in various places of the brain, brains vary structurally from patient to patient, brains vary functionally from patient to patient, and different regions of the brain are epileptic from patient to patient.&lt;/p&gt;
&lt;p&gt;TVB at its core is a model of the brain that utilizes realistic brain geometry, connectivity and clinical hypotheses to simulate electrophysiological signals. Based on the model, it can generate different patterns of behavior, or in our case, epileptic seizures. This is extremely important because as a scientist, you can control the variables to produce different types of data, but all demonstrating epileptic seizing. This hypothetically would give you a huge amount of variable data, that has ground truth set by you, and also only require computing power to simulate. It is not constrained by medical procedures and could help in generating data that deep learning models can learn from for each patient BEFORE surgery. This could help establish a completely computerized pipeline for helping clinicians make informed decisions and hypotheses before the patient is operated on with the help of deep learning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Summary of Work&lt;/strong&gt;
Using a batch of real recording data from patients, we can construct a deep learning model that learns some useful classification from the data. For example, which region is "epileptic", or when is the patient seizing? However, in order to be fair, your training of the model will always use a leave-one-patient out scheme because you have to assume you do not have access to a patient's data before making predictions on that specific patient. This brings up a lot of problems though! Even if we have "a lot" of data, there are so many clinical variables (brought up earlier) that can cause variations in how the test patient's data will look compared to your training data.&lt;/p&gt;
&lt;p&gt;So my proposal is to use TVB as a way of constructing the personalized brain model for a patient, and then simulating a highly variable dataset by varying parameters. Although it is highly possible that parts of this dataset is not fully realistic or representative of real data, it gives a way of generating significant amounts of data and a way of generating data points that are closer representations in terms of the patient's specific brain structure, connections, and electrode implantation schema. &lt;/p&gt;
&lt;p&gt;I test the potential for this by performing a set of experiments using a standard neural network trained only on the real data, vs trained on the real data AND the simulated dataset from the test patient's specific brain model. Initial results show improved convergence of training, but more work needs to be done.&lt;/p&gt;
&lt;h1&gt;Conclusions / Future Considerations&lt;/h1&gt;
&lt;p&gt;Returning to JHU, I have a lot of things to accomplish before being able to graduate with my PhD. Although spending this year may have increased my timeline to graduation, it also expanded my research scope and scientific knowledge. This is exciting to me because a PhD is not just about quickly graduating, but about developing a broad range of deep skills that allow you to make impacts on a variety of different problems.&lt;/p&gt;
&lt;p&gt;Epilepsy is only a small subset of the many unsolved neurological disorders out there. By tackling a very specific problem in the brain, I will hope to build fundamental understanding that translates to understanding other brain disorders. Specifically in the future, I am interested in Alzheimers disease.&lt;/p&gt;
&lt;h1&gt;Random Notes on This Year, Data and PhD Research&lt;/h1&gt;
&lt;p&gt;This year, I really had to deal with more data then I was accustomed to at JHU. At JHU, I had access to various text files, EEG recordings and MRI/CT imaging data for various patients from various clinical centers. However, these recordings were never longer then 5-10 minutes. &lt;/p&gt;
&lt;p&gt;Here, I had to begin dealing with data at larger scales. I had to understand how to optimize parallelized runs of linear algorithms on said data. My datasets went from a couple hundred MB (i.e. few recordings for a single patient), to a few GB (i.e. multiple patients) to a couple hundred GB and few TBs (i.e. &amp;gt;50 patients with multiple recordings). My analysis and data pipeline design scaled at the same time, but required me to refactor and understand how to continuously analyze the data robustly and efficiently. I complain a lot about having to refactor code (since it's not really research), but I think there is a lot to be learned by having to independently design, implement and test your own data pipelines as your data/analysis becomes more and more complex.&lt;/p&gt;
&lt;p&gt;I started out with analyzing datasets on my laptop/workstation with unoptimized code. Then I proceeded to optimize different parts of my workflow and moduralize it, so that it could be parallelized onto multiple cores. I then proceeded to submit it onto the JHU Maryland High-Performance Computing cluster. At the same time, I ended up learning a lot more about Unix and terminal-based commands.&lt;/p&gt;
&lt;h2&gt;Data Pipeline Design&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;First, I began with one CPU per window of data (each dataset was split into a number of windows to analyze). This resulted in hundreds-thousands of CPUs being fired up with loading the data and then analyzing a short segment of the dataset. This had numerous problems. One CPU out of hundreds/thousands could easily fail the run the job correctly, which would result in a missing computed window.&lt;/li&gt;
&lt;li&gt;Then, I began using a parallel framework with GNU that ran each dataset with a fixed 24 cores per node. This helped because GNU allows you to restart jobs using a log file, but this was also problematic because as datasets grew longer, it wasn't clear how long I would have to wait to get data per each patient.&lt;/li&gt;
&lt;li&gt;Now, I also break up datasets into chunks and then analyze using 24-48 cores at a time that sift through the data. These data analyzed-chunks can later be combined to form into the one dataset if necessary, but a metadata json object is used to store information allowing anyone using the data to understand how to put computations together.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;PhD and Research Understanding&lt;/h2&gt;
&lt;p&gt;A lot of this year was spent reading papers and doing a lot of thinking... Not very productive, but I really felt like I learned a lot and expanded my research mindset.&lt;/p&gt;
&lt;p&gt;Couple things I started learning more about:&lt;/p&gt;
&lt;h4&gt;1. reading papers is good to do on a consistent basis, but focus on getting to the core of the "key" papers (how you decide which papers are key comes with exp)&lt;/h4&gt;
&lt;p&gt;I realized that some papers are great to learn and skim through the results and methods to understand how they did it, what are the limitations and what was innovated on. Then there are other papers that introduce a completely new concept that might be different. These papers are important to understand because they usually spur papers down the road that require you to understand this one. I tend to go through the figures and equations multiple times to understand the details of the motivation, methods and results.&lt;/p&gt;
&lt;h4&gt;2. software engineering is very important in data analysis.&lt;/h4&gt;
&lt;p&gt;It helps you formulate and design a software package that is intended to "experiment" the different parameters, datasets, and visualize results in an end-to-end fashion; it ideally would allow you to "press run" while you read papers, go out drinking, or go traveling. I probably refactored my code around 5-6 times this year, which was a great learning experience, but it took a lot of time. It helped me understand the scope of my projects and will hopefully be helpful down the road in my ambition to become a data and machine learning expert.&lt;/p&gt;
&lt;h4&gt;3. it's easy to get stuck in a loop of feeling like "you're not going anywhere".&lt;/h4&gt;
&lt;p&gt;Research takes time and whether it's analyzing data, thinking of a math problem, understanding an experiment, or testing a computational model, it can become easy to think you're making no progress. It is extremely important to set mini-goals (e.g. weekly) on top of your milestone goals (e.g. monthly, or few months). You want to also allow yourself room to learn how to set these goals realistically. At the beginning, you'll think that you are capable of accomplish ABC...Z in one week. Most of the time, this ends up being overly optimistic. As you grow, you start to realize what is realistically accomplishable on a week-to-week basis. This helps you scope out each week and plan accordingly to make incremental progress.&lt;/p&gt;
&lt;p&gt;Even if your results don't pan out, this helps build a mindset of systematic thinking. You want to plan mini-experiments on your analysis that will provide you with the next step to pursue. The analysis did not work the way you expect? Okay, then we probably need to test these factors and visualize data over the next week. Okay if those factors will take too long, we should back up and clean up our approach.&lt;/p&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;http://www.thevirtualbrain.org/tvb/&lt;/li&gt;
&lt;li&gt;https://ieeexplore.ieee.org/document/7963378/&lt;/li&gt;
&lt;li&gt;https://www.ncbi.nlm.nih.gov/pubmed/29060480&lt;/li&gt;
&lt;/ol&gt;</content><category term="Academic"></category><category term="tvb"></category><category term="phd"></category></entry><entry><title>Using FreeSurfer</title><link href="/blog/2017/12/using-freesurfer/" rel="alternate"></link><published>2017-12-07T00:00:00-05:00</published><updated>2017-12-07T00:00:00-05:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-12-07:/blog/2017/12/using-freesurfer/</id><summary type="html">&lt;p&gt;To guide the user in how to setup freesurfer correctly.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Necessary Tools&lt;/h1&gt;
&lt;p&gt;Freesurfer, FSL, and MRtrix3 are the three main neuroimaging and registration software that you need to run a systematic data pipeline of neuroimaging data (i.e. CT, MRI, DWI).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Freesurfer
https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;FreeSurfer is a software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;FSL
https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MRtrix3
http://mrtrix.readthedocs.io/en/latest/installation/mac_install.html&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MRtrix3 is primarily intended to be used for the analysis of diffusion MRI data. In addition, at its fundamental level it is designed as a general-purpose library for the analysis of any type of MRI data. &lt;/p&gt;
&lt;h1&gt;Data Processing Pipeline:&lt;/h1&gt;
&lt;h2&gt;1. DWI Processing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Process diffusion imaging data, by denoising the data, then preprocessing it, then applying bias correction and then estimating the response function. &lt;/p&gt;
&lt;p&gt;dwidenoise &lt;/DTI_dicom_dir/&gt; &lt;dti_img_denoise.mif&gt;&lt;/p&gt;
&lt;p&gt;dwipreproc -rpe_none AP DTI_30_average-2_denoise.mif &lt;dti_img_preproc_output.mif&gt;&lt;/p&gt;
&lt;p&gt;dwibiascorrect &lt;dti_img_preproc_output.mif&gt; &lt;dti_img_preproc_biascorrect_output.mif&gt; â€“fsl&lt;/p&gt;
&lt;p&gt;dwi2response tournier &lt;dti_img_preproc_biascorrect_output.mif&gt; &lt;dti_img_preproc_biascorrect_response.txt&gt;&lt;/p&gt;
&lt;p&gt;dwi2fod csd DTI_30_average-2_denoise_preproc_biascorrected.mif DTI_30_average-2_denoise_preproc_biascorrected_response.txt DTI_30_average-2_denoise_preproc_biascorrected_fod.mif&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2. T1 MRI Processing&lt;/h2&gt;
&lt;h2&gt;3. (Optional) CT Processing&lt;/h2&gt;
&lt;h2&gt;4. Connectome Generation&lt;/h2&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Freesurfer is a tool built for rendering 3D brains using MRI and Ct scans. FSL is a tool for coregistration and image analysis. Download both online:&lt;/p&gt;
&lt;h2&gt;Common Definitions:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Registration: to find a common coordinate system for the input data sets&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;
&lt;p&gt;First, let's walk through how implementation occurs.&lt;/p&gt;
&lt;h2&gt;1. Set Up&lt;/h2&gt;
&lt;p&gt;First you will want to download FreeSurfer from the following website:
https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall
Linux:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;export&lt;/span&gt; &lt;span class="n"&gt;FREESURFER_HOME&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;freesurfer&lt;/span&gt;
&lt;span class="k"&gt;source&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;FREESURFER_HOME&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;SetUpFreeSurfer&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sh&lt;/span&gt;

&lt;span class="o"&gt;##&lt;/span&gt; &lt;span class="n"&gt;tcsh&lt;/span&gt;
&lt;span class="n"&gt;setenv&lt;/span&gt; &lt;span class="n"&gt;FREESURFER_HOME&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;freesurfer&lt;/span&gt;
&lt;span class="k"&gt;source&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;FREESURFER_HOME&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;SetUpFreeSurfer&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;csh&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Mac:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;export FREESURFER_HOME=/Applications/freesurfer&lt;/span&gt;
&lt;span class="err"&gt;source $FREESURFER_HOME/SetUpFreeSurfer.sh&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Run those commands within your terminal, or add them to &lt;code&gt;~/.bashrc&lt;/code&gt; file to have access to all the command line tools for freeview.&lt;/p&gt;
&lt;p&gt;You will need to setup your own directory where you will hold all your subject data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;export SUBJECTS_DIR=&amp;lt;path to subject data&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will be where you store say patient's MRI/CT scans that you will use to reconstruct the brain.&lt;/p&gt;
&lt;h2&gt;2. Running Through T1-Weighted MRI Images&lt;/h2&gt;
&lt;p&gt;First you want to set your current Subjects directory to where you are working with the raw say .dcm data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;export SUBJECTS_DIR=$PWD&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There may be an issue where you can't run the functions with your .dcm files. I have no idea why this occurs, but an easy fix is to externally run a dicom to nii converter and pass this type of file instead. Here is a link to a matlab converter that can do this:
https://www.mathworks.com/matlabcentral/fileexchange/42997-dicom-to-nifti-converter--nifti-tool-and-viewer&lt;/p&gt;
&lt;p&gt;There are also other resources online.&lt;/p&gt;
&lt;p&gt;Afterwards, you can run the following command(s):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;recon-all -i &amp;lt;data&amp;gt;.nii -s &amp;lt;subject_name&amp;gt; -autorecon1&lt;/span&gt;
&lt;span class="err"&gt;recon-all -i &amp;lt;data&amp;gt;.nii -s &amp;lt;subject_name&amp;gt; -all&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will take a long time! So be prepared to run this on a compute engine that has time.&lt;/p&gt;
&lt;h2&gt;3. Running Through CT Images&lt;/h2&gt;
&lt;p&gt;flirt -in patient_ct.nii -ref patient_mri.nii -omat patient_omat.mat -out patient_registered.nii.gz&lt;/p&gt;
&lt;p&gt;This command will coregister the CT data onto the domain of the MRI data and provide a coregistration for you to look at where certain contacts of electrodes are. You can also view in Freeview the different cuts of the brain using either CT, or MRI. &lt;/p&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#Setup.26Configuration&lt;/li&gt;
&lt;/ol&gt;</content><category term="Academic"></category><category term="data analysis"></category><category term="eeg"></category><category term="phd"></category><category term="brain rendering"></category></entry><entry><title>Simulating Epileptic iEEG Activity Using The Virtual Brain</title><link href="/blog/2017/09/simulating-tvb/" rel="alternate"></link><published>2017-09-27T00:00:00-04:00</published><updated>2017-09-27T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-09-27:/blog/2017/09/simulating-tvb/</id><summary type="html">&lt;p&gt;To guide the simulation of Epileptic iEEG activity using TVB in Marseille, France.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;TVB is a platform for simulating whole-brain dynamics that starts from raw data involving:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;structural connectivity derived from DTI&lt;/li&gt;
&lt;li&gt;brain parcellation derived from MRI and CT&lt;/li&gt;
&lt;li&gt;SEEG xyz locations derived from MRI and CT&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This will then determine a gain matrix to determine SEEG signals from the source signals that are 
generated from neural mass models. The neural mass models will be implemented 
with nonlinear, complex models for simulating certain type of electrophysiology. 
The Epileptor is used for simulating seizure activity from a specific source region. &lt;/p&gt;
&lt;p&gt;The epileptor is a set of coupled differential equations that rely on 6 different variables. 
They are described here:&lt;/p&gt;
&lt;h1&gt;Data &amp;amp; Metadata&lt;/h1&gt;
&lt;p&gt;Generally, refer to my post on &lt;code&gt;Freesurfer&lt;/code&gt; to establish the preprocessing 
data pipeline using Freesurfer, FSL and MRtrix3.&lt;/p&gt;
&lt;p&gt;The minimum necessary requirements for creating the TVB dataset are a set 
of T1 and DWI images as a list of dicom files, or a single 4-D image nifti file.&lt;/p&gt;
&lt;p&gt;A high level summary of how the pipeline proceeds is:
1. Construct Cortical Surface, Subcortical Surface&lt;/p&gt;
&lt;p&gt;Using freesurfer, you can get the reconstructed surfaces, 
which are your files that outline the voxels that belong 
to each region of the brain. This will give you the surface 
geometries of the cortical and subcortical surface.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Construct Parcellation Scheme&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This can range from the default in freesurfer to different 
atlases available for the human brain. This will give you 
a region mapping for every vertex/face from your cortical/subcortical
surface geometries files.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Construct Corticography Tracts&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First, you need to coregister the DWI images with the T1 scans 
Using the DWI images, along with the reconstructed surfaces, 
you can count fiber tracts between each region of the brain 
and reconstruct the structural connectivity matrices. This is composed from the weights matrix and the length matrix between parcellated regions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Obtain Electrode Coordinates in T1 Space&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First, you need to coregister the CT reconstructed freesurfer file into the T1 space. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Computing Gain Matrix Between Brain Regions and Electrodes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to compute forward solutions of electrode (i.e. SEEG, ECoG, etc.) activity, 
you need to compute a gain matrix that transforms region activity into electrode activity. 
This can be done using an inverse-square method fall-off on the region activity, 
or using a dipole method as outlined in the "Virtual Epileptic Patient" paper.
Note that these methods have some limitations, as it assumes all 
activity from brain regions are "projected" to the iEEG electrodes 
instantaneously.&lt;/p&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;
&lt;h2&gt;1. Setting Up Environment&lt;/h2&gt;
&lt;p&gt;First you may want to set up a conda environment, or a virtualenv that will separate the entire python project from your normal OS.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;pip install nibabel networkx&lt;/span&gt;
&lt;span class="err"&gt;git clone https://github.com/the-virtual-brain/tvb-data&lt;/span&gt;
&lt;span class="err"&gt;git clone https://github.com/the-virtual-brain/tvb-library&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you want to have a script to add these all to path for your jupyter notebook, use the following:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7
8
9&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Launching IPython Notebook from TVB Distribution&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -z &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$LANG&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LANG&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;en_US.UTF-8
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LC_ALL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$LANG&lt;/span&gt;
&lt;span class="c1"&gt;# add tvb data and library to path and launch notebook&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PYTHONPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;/_tvbdata:&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;/_tvblibrary:&lt;span class="nv"&gt;$PYTHONPATH&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;h2&gt;1b. Setting Up Environment on a Cluster&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;. /soft/miniconda3/activate&lt;/span&gt;
&lt;span class="err"&gt;conda env list&lt;/span&gt;
&lt;span class="err"&gt;conda create -n tridesclous python=3.6 scipy numpy pandas scikit-learn matplotlib seaborn pyqt=5 ipykernel&lt;/span&gt;
&lt;span class="err"&gt;source activate tridesclous&lt;/span&gt;
&lt;span class="err"&gt;pip install pyqtgraph&lt;/span&gt;
&lt;span class="err"&gt;pip install https://github.com/tridesclous/tridesclous/archive/master.zip&lt;/span&gt;
&lt;span class="err"&gt;python -m ipykernel install --name tridesclous-testing â€”user&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;2. Simulating Epilepsy&lt;/h2&gt;
&lt;p&gt;In order to simulate epilepsy, you are going to walk through a pipeline 
using TVB. Details are left out for now.&lt;/p&gt;
&lt;p&gt;i. Structural Connectivity&lt;/p&gt;
&lt;p&gt;What is the matrix of connectivities between your brain regions?
Ex: Connectivity weights, conduction speed, coupling function between long-range regions&lt;/p&gt;
&lt;p&gt;ii. Neural Mass Model&lt;/p&gt;
&lt;p&gt;What is the phenomenological model at brain regions?
Ex: Epileptor6D, with parameter settings&lt;/p&gt;
&lt;p&gt;iii. Integrators&lt;/p&gt;
&lt;p&gt;How to solve your stochastic differential equation?
Ex: Heunstochastic, with noise levels&lt;/p&gt;
&lt;p&gt;iv. Coupling&lt;/p&gt;
&lt;p&gt;How are your brain regions coupled? How does activity 
of one region translate to the next? Is it transformed 
via a function?
Ex: linear, additive, hyperbolic&lt;/p&gt;
&lt;p&gt;v. Monitors&lt;/p&gt;
&lt;p&gt;What variables to monitor and store?
Ex: State variables, iEEG activity from sampling 
rate and period.&lt;/p&gt;
&lt;p&gt;Then once these are complete, you can run your simulation.&lt;/p&gt;
&lt;h2&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;https://github.com/the-virtual-brain/tvb-library&lt;/li&gt;
&lt;li&gt;https://github.com/the-virtual-brain/tvb-epilepsy&lt;/li&gt;
&lt;li&gt;https://www.thevirtualbrain.org/tvb/zwei&lt;/li&gt;
&lt;/ol&gt;</content><category term="Academic"></category><category term="data analysis"></category><category term="eeg"></category><category term="phd"></category><category term="computational modeling"></category></entry><entry><title>Doctoral Board Oral Exam (PhD)</title><link href="/blog/2017/08/doctoral-board-oral/" rel="alternate"></link><published>2017-08-05T00:00:00-04:00</published><updated>2017-08-05T00:00:00-04:00</updated><author><name>Adam Li</name></author><id>tag:None,2017-08-05:/blog/2017/08/doctoral-board-oral/</id><summary type="html">&lt;p&gt;A short walkthrough of my experience with the DBO exam at Johns Hopkins University&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Doctoral Board Oral Exam&lt;/h1&gt;
&lt;!-- MarkdownTOC autolink="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-cable-theory-and-compartmental-modeling"&gt;1. Cable Theory and Compartmental Modeling&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#examples-of-different-types-of-neurons"&gt;Examples of different types of neurons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#approach"&gt;Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#important-equations"&gt;Important Equations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#unsealed-end"&gt;Unsealed end:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sealed-end"&gt;Sealed end:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#concatenated-cables"&gt;Concatenated Cables:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#definitions"&gt;Definitions:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#compartmental-models"&gt;Compartmental Models:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#study"&gt;Study&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-generalized-linear-models"&gt;2. Generalized Linear Models:&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#general-form-of-exponential-family-distribution"&gt;General Form of Exponential Family Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#normal-linear-regression"&gt;Normal Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#logistic-regression"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#poisson-regression"&gt;Poisson Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#solving-glm-methods"&gt;Solving GLM Methods:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#goodness-of-fit"&gt;Goodness of Fit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-kalman-filter"&gt;3. Kalman Filter&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#stochastic-state-space-model"&gt;Stochastic State-Space Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#assumptions"&gt;Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#derivation"&gt;Derivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#relation-to-a-least-squares-problem"&gt;Relation to a Least-Squares Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#relation-to-a-bayesian-maximum-aposteri-estimation"&gt;Relation to a Bayesian Maximum Aposteri Estimation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-expectation-maximization"&gt;4. Expectation Maximization&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#basic-idea"&gt;Basic Idea&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-k-means-algorithm"&gt;5. K-Means Algorithm&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#cost-function"&gt;Cost Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-algorithm"&gt;The Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#initialization"&gt;Initialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#convergence"&gt;Convergence&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-sensory-pathways-and-systems-in-neuroscience"&gt;6. Sensory Pathways and Systems in Neuroscience&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-motor-pathways-and-systems-in-neuroscience"&gt;7. Motor Pathways and Systems in Neuroscience&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-general-topics-in-neuroscience"&gt;8. General Topics in Neuroscience&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#9-gaussian-mixture-models"&gt;9. Gaussian Mixture Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;
&lt;h1&gt;1. Cable Theory and Compartmental Modeling&lt;/h1&gt;
&lt;h2&gt;Examples of different types of neurons&lt;/h2&gt;
&lt;p&gt;There are thalamic cells, pyramidal cells, double pyramidal cells, granule cells, purkinje cells. These all have different morphologies and perform different computations.&lt;/p&gt;
&lt;h2&gt;Approach&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Approximate dendrites as uniform membrane cylandiers&lt;/li&gt;
&lt;li&gt;Synaptic inputs are approximated as 'injected currents'&lt;/li&gt;
&lt;li&gt;Use the cable equation to create a system of differential equations for each cylinder.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Important Equations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;input conductance of semi-infinite cable&lt;/li&gt;
&lt;li&gt;input conductance of infinite cable&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Unsealed end:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;cable equation&lt;/li&gt;
&lt;li&gt;input conductance of finite cable&lt;/li&gt;
&lt;li&gt;s.s. voltage along cable&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Sealed end:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;input conductance of the finite cable&lt;/li&gt;
&lt;li&gt;s.s. voltage along cable&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Concatenated Cables:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Compute V(X) along branches, by determining &lt;span class="math"&gt;\(G_{out}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Definitions:&lt;/h3&gt;
&lt;p&gt;Know definitions and related biology of:
- axial resistance, the resistance to current flow along the uniform cable (along the dendrite)
- membrane resistance, the resistance of current flow out of the dendrite
- membrance capacitance, the capacitance of a patch of membrane surface area
- derive cable equation as a model of concatenated RC circuits&lt;/p&gt;
&lt;h3&gt;Compartmental Models:&lt;/h3&gt;
&lt;p&gt;Be able to derive system of equations into a matrix form for a linear time-invariant system &lt;/p&gt;
&lt;p&gt;Understand how transfer resistances work. Understand how distributing synaptic inputs works. &lt;/p&gt;
&lt;p&gt;Understand how coincidence detection (AND operator), shunting inhibition (AND-NOT operator).&lt;/p&gt;
&lt;h2&gt;Study&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Definitions of cable theory and biological relations&lt;/li&gt;
&lt;li&gt;Solving single cable equation and different boundary conditions&lt;/li&gt;
&lt;li&gt;Derivation of cable equation under different boundary conditions&lt;/li&gt;
&lt;li&gt;Deriving LTI system from compartmental models of cables&lt;/li&gt;
&lt;li&gt;Derive transfer resistances&lt;/li&gt;
&lt;li&gt;Derive AND operator (coincidence detection)&lt;/li&gt;
&lt;li&gt;Derive AND-NOT operator (shunting inhibition)&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;2. Generalized Linear Models:&lt;/h1&gt;
&lt;h2&gt;General Form of Exponential Family Distribution&lt;/h2&gt;
&lt;p&gt;Be able to define all the terms, such as: natural parameters, sufficient statistic, natural link function, dispersion parameter&lt;/p&gt;
&lt;h2&gt;Normal Linear Regression&lt;/h2&gt;
&lt;h2&gt;Logistic Regression&lt;/h2&gt;
&lt;h2&gt;Poisson Regression&lt;/h2&gt;
&lt;h2&gt;Solving GLM Methods:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Penalized Quasi-likelihood&lt;/li&gt;
&lt;li&gt;Laplace's Method&lt;/li&gt;
&lt;li&gt;Adaptive Gaussian Quadrature&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Goodness of Fit&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Chi-square test&lt;/li&gt;
&lt;li&gt;Kolmogorov Statistic&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;3. Kalman Filter&lt;/h1&gt;
&lt;h2&gt;Stochastic State-Space Model&lt;/h2&gt;
&lt;p&gt;Here, list the linear, time-invariant system with a state evolution equation and measurement/observation equation.&lt;/p&gt;
&lt;h2&gt;Assumptions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;state and observation noises are independent, zero-mean Gaussian white processes with some defined covariances&lt;/li&gt;
&lt;li&gt;initial state x_0 is a Gaussian R.V. independent of the state/observation noises&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Derivation&lt;/h2&gt;
&lt;p&gt;Derive the Kalman filter equations for the state update, covarariance estimates&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Measurement update&lt;/li&gt;
&lt;li&gt;Time update&lt;/li&gt;
&lt;li&gt;Combined Update&lt;/li&gt;
&lt;li&gt;Covariance Update&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Relation to a Least-Squares Problem&lt;/h2&gt;
&lt;p&gt;Review...&lt;/p&gt;
&lt;h2&gt;Relation to a Bayesian Maximum Aposteri Estimation&lt;/h2&gt;
&lt;p&gt;Review...&lt;/p&gt;
&lt;h1&gt;4. Expectation Maximization&lt;/h1&gt;
&lt;h2&gt;Basic Idea&lt;/h2&gt;
&lt;p&gt;By computing the likelihood of your unknown parameters, based on known outcomes. You can perform maximum likelihood estimation to get the best estimate of the unknown parameters&lt;/p&gt;
&lt;h1&gt;5. K-Means Algorithm&lt;/h1&gt;
&lt;h2&gt;Cost Function&lt;/h2&gt;
&lt;p&gt;The cost function is attempting to minimize the distortion (distance to centers) for every point in the set of data points, S.&lt;/p&gt;
&lt;h2&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;'''
Input: k clusters
initialize centers: z_1, ..., z_k \in \real^d and clusters C_1, ..., C_k
repeat until there is no further change in L(z, C):
    for each j (data point): C_j &amp;lt;- {x \in S whose closest center is z_j}
    for each j (data point): z_j &amp;lt;- mean(C_j) 
'''&lt;/p&gt;
&lt;p&gt;Walk through for a small example to see how the algorithm works:
(0, 0, 0)
(0, 0.5, 1)
(1, 3, 2)
(4, 5, 6)
(2, 3, 1)
(5, 2, 0)
(0, 1, 0)
(1, 1, 0)
(2, 1, 0)&lt;/p&gt;
&lt;h2&gt;Initialization&lt;/h2&gt;
&lt;p&gt;Initialization of a k-means type algorithm initializes k "means", we call centers at the beginning, and then assigns points into these centers based on the cost function. &lt;/p&gt;
&lt;h2&gt;Convergence&lt;/h2&gt;
&lt;p&gt;Show that the cost monotonically decreases, so the algorithm will converge at least in the sense of cost decreasing to a non-changing amount.&lt;/p&gt;
&lt;h1&gt;6. Sensory Pathways and Systems in Neuroscience&lt;/h1&gt;
&lt;p&gt;The different neuronal pathways in the central nervous system, vs the periphery nervous system. &lt;/p&gt;
&lt;p&gt;Visual sensory pathway that leads from the retina all the way to the occipital lobe of the brain.&lt;/p&gt;
&lt;p&gt;The somatosensory pathways that lead to the SI, and SII all with nerve sensors from the periphery of your body.&lt;/p&gt;
&lt;h1&gt;7. Motor Pathways and Systems in Neuroscience&lt;/h1&gt;
&lt;p&gt;How does motor movements get controlled in the brain?&lt;/p&gt;
&lt;p&gt;The premotor cortex, and also the motor cortex are at the highest levels of control. We can also include the cerebellum when we talk about motor control.&lt;/p&gt;
&lt;h1&gt;8. General Topics in Neuroscience&lt;/h1&gt;
&lt;p&gt;Good things to know and remember are:
- Hodgkin Huxley models: a biophysical model that models realistic neuronal firing based on concentration gradients, membrane conductances, membrane potentials and even more.
- Nernst equation: a fundamental equation for determining the resting potential of a membrane in the presence of a concentration gradient of an ion.
- Goldman-Katz equation: a model with multiple ions that determines the resting potential of a membrane (extension of Nernst)
- Action potentials and the chronology of cell actions that are taken&lt;/p&gt;
&lt;h1&gt;9. Gaussian Mixture Models&lt;/h1&gt;
&lt;p&gt;This is the idea that some data distributions can be the combinations of many different Gaussian models with different means and variance, but superimposed on each other. &lt;/p&gt;
&lt;p&gt;The goal of a gaussian mixture model is to identify the parameters of each of the separate gaussians by a process called Expectation Maximization. In this process, we switch between maximizing the likelihood by iterating on the parameters of the gaussian models and computing a new likelihood function given the new parameters.&lt;/p&gt;
&lt;h1&gt;FAQ&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Q: How long should I study for?
A: I would study for about 2-3 weeks with nothing else going on. You want to be able to do a comprehensive overview of everything first, and then dive deeper into topics that are more likely to come up. You should also do test runs of walking through "open-ended" questions on a white board many times.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The goal here is to practice thinking out loud and being very clear in your communication.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Academic"></category><category term="doctoral board oral"></category><category term="phd"></category><category term="johns hopkins"></category></entry></feed>