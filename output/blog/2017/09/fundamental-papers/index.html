<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Important Papers for Fundamentals in Computational Neuroscience / Data Science</title>
        <link rel="stylesheet" href="https://adam2392.github.io/theme/css/main.css" />
        <link href="https://adam2392.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Adam Li's blog Atom Feed" />
        <link href="https://adam2392.github.io/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Adam Li's blog RSS Feed" />
</head>

<body id="index" class="home">
<a href="https://github.com/adam2392">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="https://adam2392.github.io/">Adam Li's blog </a></h1>
                <nav><ul>
                    <li><a href="/categories.html">Blog</a></li>
                    <li><a href="/archives.html">Timeline</a></li>
                    <li><a href="/tags.html">Tags</a></li>
                    <li><a href="/pdfs/AdamLi_CV.pdf">Curriculum Vitae</a></li>
                    <li><a href="https://adam2392.github.io/contact/">Contact</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="https://adam2392.github.io/blog/2017/09/fundamental-papers/" rel="bookmark"
           title="Permalink to Important Papers for Fundamentals in Computational Neuroscience / Data Science">Important Papers for Fundamentals in Computational Neuroscience / Data Science</a></h1>
<a href="https://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="adam2392">Tweet</a><script type="text/javascript" src="https://platform.twitter.com/widgets.js"></script>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-09-25T00:00:00-04:00">
                Published: Mon 25 September 2017
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://adam2392.github.io/author/adam-li.html">Adam Li</a>
        </address>
<p>In <a href="https://adam2392.github.io/category/academic.html">Academic</a>.</p>
<p>tags: <a href="https://adam2392.github.io/tag/phd.html">phd</a> <a href="https://adam2392.github.io/tag/journals.html">journals</a> <a href="https://adam2392.github.io/tag/reviews.html">reviews</a> </p>
</footer><!-- /.post-info -->      <!-- MarkdownTOC -->

<ul>
<li>Papers<ul>
<li>
<ol>
<li>Wilson-Cowan Neural Mass Model</li>
<li>Summary / Conclusions:</li>
<li>Important Notes:</li>
</ol>
</li>
<li>
<ol>
<li>Kalman Filter Model</li>
<li>Summary / Conclusions:</li>
<li>Important Notes:</li>
</ol>
</li>
<li>
<ol>
<li>Expectation Maximization</li>
<li>Summary / Conclusions:</li>
<li>Important Notes:</li>
</ol>
</li>
<li>
<ol>
<li>Information Bottleneck Method</li>
<li>Summary / Conclusions:</li>
<li>Important Notes:</li>
</ol>
</li>
<li>
<ol>
<li>Opening the Black Box of Deep Neural Networks Via Information</li>
<li>Summary / Conclusions:</li>
<li>Important Notes:</li>
</ol>
</li>
<li>
<ol>
<li>Reinforcement Learning: An Overview</li>
<li>Summary / Conclusions:</li>
<li>Important Notes:</li>
</ol>
</li>
<li>
<ol>
<li>Deep Reinforcement Learning: An Overview</li>
</ol>
</li>
<li>
<ol>
<li>Variational Inference:</li>
<li>Summary / Conclusions</li>
<li>Important Notes</li>
</ol>
</li>
</ul>
</li>
</ul>
<!-- /MarkdownTOC -->

<h1>Papers</h1>
<h2>1. Wilson-Cowan Neural Mass Model</h2>
<h3>Summary / Conclusions:</h3>
<h3>Important Notes:</h3>
<h2>2. Kalman Filter Model</h2>
<h3>Summary / Conclusions:</h3>
<h3>Important Notes:</h3>
<h2>3. Expectation Maximization</h2>
<h3>Summary / Conclusions:</h3>
<h3>Important Notes:</h3>
<h2>4. Information Bottleneck Method</h2>
<p>http://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf</p>
<h3>Summary / Conclusions:</h3>
<p>Let us define X as input signal, and Y as desired output.</p>
<p>Here, they were interested in deriving a quantitative method for optimizing 1) compression rate of a signal and 2) the choice of representation of the original signal.</p>
<p>Previous theory looked at minimizing the rate of compression given a constraint on expected distortion of the original signal (with new compression). This was solved via iterative algorithm (similar to EM), but lacked generality to find optimal representatives, which minimize the expected distortion (not just compression) of the signal. </p>
<p>The new theory looks at minimizing the rate of compression given a constraint on the amount of information we can keep about Y using X. This produces an iterative algorithm that also can be solved iteratively. It also shows that 1) the Kullback-Leibler divergence is the relevant distortion measure for the information bottleneck setting, and 2) optimization of representation of signal and the signal compression can be done together.</p>
<p>This work can be used in applications to information processing problems (e.g. deep learning).</p>
<h3>Important Notes:</h3>
<p>Mutual information is defined as:</p>
<p>$I(X;Y) = D_{KL}[P(x,y)||P(x)P(y)] = \sum_{x\in X, y\in Y} P(x,y) log(\frac_{P(x,y)}<em X_="X," Y x_in="x\in" y_in="y\in">{P(x)P(y)})$
$= \sum</em>P(x,y) log(\frac_{P(x|y)}_{P(x)}) = H(X) - H(X|Y)$</p>
<p>, where $D_{KL}(p||q)$ is the Kullback-Liebler divergence of distributions p and q, and $H(X)$ and $H(X|Y)$ are entropy and conditional entropies, respectively.</p>
<p>We want the optimal representations of signal X with respect to output label Y. Sufficient statistics are maps/partitions of X, S(X) that captures all the information X has about Y. The mutual information given Y is equal. </p>
<p>$I(S(X); Y) = I(X; Y)$</p>
<p>We can allow the map to be stochastic, with encoder P(T|X) and allow map to capture as much as possible of I(X;Y), not necessarily all of it $I(S(X); Y) \leq I(X; Y)$. Define $t \in T$ as compressed representations of $x \in X$, stochastically, $p(t|x)$. The following optimization problem finds a balance between compression of X and prediction of Y.</p>
<p>$min_{p(t|x),p(y|t),p(t)}\ {I(X;T) - \beta I(T;Y)}$</p>
<h2>5. Opening the Black Box of Deep Neural Networks Via Information</h2>
<p>Reference: https://arxiv.org/pdf/1703.00810.pdf</p>
<h3>Summary / Conclusions:</h3>
<p>In this paper, the authors extend their analysis of DNN using information theory. They answered the following questions:</p>
<ol>
<li>The SGD layer dynamics in the Information plane.
First the layers increase $I(T_i;Y)$, and then later decrease $I(X; T_i)$, which corresponds to increasing the information about Y and then later compressing the representation (empirical error minimization &amp; representation compression phase).</li>
<li>The effect of the training sample size on the layers.
It seems that sample size does not have an effect on empirical error minimization, but does have an effect on the representation compression. Smaller sample sizes has overfitting, which has been seen as overfitting the sample noise. </li>
<li>
<p>What is the benefit of the hidden layers?
Adding hidden layers reduces the number of training epochs. It also seems to accelerate compression, but adding extra width does not seem to help. With layered diffusion of the SGD optimization (backpropagation), it seems that there is an exponential decrease in epochs with K hidden layers.</p>
</li>
<li>
<p>What is the final location of the hidden layers?</p>
</li>
<li>
<p>Do the hidden layers form optimal IB representations?
It seems that the hidden layers converge to the optiaml IB representations. However, we can see that there can be clearly many different layers that correspond to the same IB representation.</p>
</li>
</ol>
<p>Another important hypothesis/conjecture they make is that: attempts to interpret single weights, or even single neurons in such networks can be meaningless because there is a large number of different networks that can achieve optimal performance. This makes sense in terms of how the brain is structured; the brain does not form the same pathway for learning something new between different people, but form a unique network for optimal performance. </p>
<h3>Important Notes:</h3>
<p>First, they estimated the mutual information of the layers with the input and with the labels $I(X;T_i)$ and $I(T_i;Y)$. </p>
<h2>6. Reinforcement Learning: An Overview</h2>
<p>Reference: https://pdfs.semanticscholar.org/b373/b0c6e3b4fef4ac0534965708fc382343f8dc.pdf</p>
<h3>Summary / Conclusions:</h3>
<h3>Important Notes:</h3>
<h2>7. Deep Reinforcement Learning: An Overview</h2>
<p>Reference:</p>
<h2>8. Variational Inference:</h2>
<h3>Summary / Conclusions</h3>
<p>Variational inference</p>
<h3>Important Notes</h3>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="https://adam2392.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>
                            <li><a href="https://adam2392.github.io/feeds/all.rss.xml" type="application/rss+xml" rel="alternate">rss feed</a></li>

                            <li><a href="https://twitter.com/adam2392">twitter</a></li>
                            <li><a href="https://stackexchange.com/users/4494355/ajl123">stack-overflow</a></li>
                            <li><a href="https://github.com/adam2392">github</a></li>
                            <li><a href="https://www.linkedin.com/in/adam2392">linkedin</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

    <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-106551801-1', 'auto');
    ga('send', 'pageview');
    </script>
</body>
</html>