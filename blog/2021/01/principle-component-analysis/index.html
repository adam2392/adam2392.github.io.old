<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Principle Component Analysis (PCA; Proper Orthogonal Decomposition) - Adam Li's blog</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


    <link href="/images/brain.jpg" rel="icon">

<link rel="canonical" href="/blog/2021/01/principle-component-analysis/">

        <meta name="author" content="Adam Li" />
        <meta name="keywords" content="phd,machine learning" />
        <meta name="description" content="An overview of principle component analysis." />

        <meta property="og:site_name" content="Adam Li's blog" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Principle Component Analysis (PCA; Proper Orthogonal Decomposition)"/>
        <meta property="og:url" content="/blog/2021/01/principle-component-analysis/"/>
        <meta property="og:description" content="An overview of principle component analysis."/>
        <meta property="article:published_time" content="2021-01-11" />
            <meta property="article:section" content="Machine Learning" />
            <meta property="article:tag" content="phd" />
            <meta property="article:tag" content="machine learning" />
            <meta property="article:author" content="Adam Li" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/native.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>

        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Adam Li's blog ATOM Feed"/>

        <link href="/feeds/all.rss.xml" type="application/rss+xml" rel="alternate"
              title="Adam Li's blog RSS Feed"/>
        <link href="/feeds/machine-learning.atom.xml" type="application/atom+xml" rel="alternate"
              title="Adam Li's blog Machine Learning ATOM Feed"/>
</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
Adam Li's blog            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/categories.html">Blog</a></li>
                    <li><a href="/pdfs/Personal_CV.pdf">Curriculum Vitae</a></li>
                         <li><a href="/gallery/">
                             Gallery
                          </a></li>
                         <li><a href="/nonprofit/">
                             Non-Profit Work
                          </a></li>
                         <li><a href="/openscience/">
                             Open Science
                          </a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<!-- Banner -->
<!-- End Banner -->

<!-- Content Container -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="/blog/2021/01/principle-component-analysis/"
                       rel="bookmark"
                       title="Permalink to Principle Component Analysis (PCA; Proper Orthogonal Decomposition)">
                        Principle Component Analysis (PCA; Proper Orthogonal Decomposition)
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2021-01-11T00:00:00-05:00"> Mon 11 January 2021</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="/tag/phd.html">phd</a>
        /
	<a href="/tag/machine-learning.html">machine learning</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <h1>Background</h1>
<p>When i.i.d. vector data is collected, then it is generally of interest to 
represent the data in as low-dimension as possible. </p>
<p>Given <span class="math">\(x_k \in \mathbb{R}^d \sim X\)</span> for k=1,...,n data points (e.g. 
if data are images, then d is the number of pixels), one would like to
simultaneously i) discover interdependencies within the data and ii) reduce dimensions. 
It is important to note here that the data is <strong>sampled</strong> from a distribution and so 
any downstream statistics (e.g. covariance) are <strong>estimates</strong> of the true 
underlying statistic.</p>
<p>Mathematically, this problem can be represented in many ways:</p>
<ol>
<li>
<p>Minimizing Cost functions</p>
<p>On the one hand we would like to find a lower-dimensional representation of our data, <span class="math">\(x_k\)</span>. 
We would like to find the minimization of the following cost function</p>
<p>
<div class="math">$$\argmin_P E[ x - Px ]^2$$</div>
</p>
<p>where P is a projection operator of rank r. P takes the data points <span class="math">\(x_k\)</span> 
and maps them from dimension d to r &lt;&lt; d.</p>
</li>
<li>
<p>Maximizing Cost Functions</p>
<p>On the other hand, we can view PCA as a method for maximizing the variance 
observed in the data, subject to a constraint. If we stack our data samples, <span class="math">\(x_k\)</span> 
column-by-column in a data matrix, X, we seek a linear combination of the columns 
that give us the <strong>maximal variance</strong>. <span class="math">\(Var(Xa) = a^T Var(X) a\)</span>, where a is a 
vector of constants to provide linear combinations of X, and <span class="math">\(Var(X)\)</span> is the 
covariance matrix of our data.</p>
<p>In addition, we impose a constraint on the vector, a, such that it has unit norm. 
Thus using the theory of Lagrange multipliers, we can write this as a 
constrained optimization problem.</p>
<p>
<div class="math">$$\argmax_a a^T Var(X) a - \lambda(a^T a - 1)$$</div>
</p>
<p>When differentiating with respect to a, and solving the equations, one 
gets the analytical expression:</p>
<p>
<div class="math">$$Var(X) a = \lambda a$$</div>
</p>
<p>which is simply an eigenvalue equation, where <span class="math">\(\lambda\)</span> is an eigenvalue of 
Var(X) and a is an eigenvector. </p>
</li>
</ol>
<h1>Standard PCA and Algorithm</h1>
<p>Standard PCA is commonly implemented in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn</a>
where the algorithm relies on the Singular Value Decomposition (SVD). Generally, 
it uses the <code>LAPACK</code> implementation, which supports full, truncated and randomized SVD. 
Conceptually the algorithm proceeds in 4 steps.</p>
<ol>
<li>
<p>Standardization of variable scaling</p>
<p>This step attempts to put all dimensions of the data in the same scale. 
In the downstream step of the SVD, for variables that have a significantly larger 
amplitude then other variables, then the variability in those variables will dominate. </p>
<p>Common standardizations include the z-scoring method:</p>
<p>
<div class="math">$$x_{k_i} = \frac{x_{k_i} - \mu(x_k)}{\sigma(x_k)}$$</div>
</p>
</li>
<li>
<p>Estimate covariance matrix</p>
<p>The next step will compute the covariance matrix of the data. 
If there are n samples of a d-dimensional data, then the covariance 
matrix will be <span class="math">\(d \times d\)</span>.</p>
</li>
<li>
<p>Matrix decomposition (SVD/EVD)</p>
<p>Since the covariance matrix is a positive semi-definite (symmetric and 
non-negative quadratic form), then its singular values are equivalent to 
its eigenvalues. The singular/eigen vectors 
of the covariance matrix are ordered by the values of the singular/eigen values.</p>
</li>
<li>
<p>Computing Principle Components</p>
<p>The principle components of the data are obtained by multiplying the data 
with the singular vector matrix. For example, the first 5 principle components corresponding to the 
5 largest singular values can be used to obtain a 5-dimensional representation 
of the original d-dimensional dataset. We simply take the <span class="math">\(d \times 5\)</span> truncated matrix 
and multiply it with our data to obtain n samples now of a 5-dimensional data.</p>
</li>
</ol>
<h1>Statistical Considerations</h1>
<h2>Independence</h2>
<p>If random variables are independent, then they have 0 correlation. Independence is a stronger 
assumption then non-correlated. Most of the times, independence is not fully realized; it is 
just a necessary assumption for mathematical modeling. However, when random variables are not necessarily independent, it is not necessary that 
they have non-zero correlation. In fact, they may have high correlation. </p>
<p>When performing PCA on correlated data, most likely there will be a few principle components
in the data that are captured.</p>
<h2>Centering</h2>
<p>When computing the principle components, it is in general common practice to 
center the columns of the data matrix first. </p>
<p>Geometrically this centers all data points around the origin. PCA attempts at finding 
an orthogonal rotation to represent the data; note that this rotation occurs 
about the origin!</p>
<p>If the mean is not subtracted, then essentially the first principle component ends up being 
the mean vector of the data points, since rotating that mean would result in higher 
mean-squared error.</p>
<h2>Compressed Sensing (n &lt; d) - "Robust PCA"</h2>
<p>When dealing with high-dimension data with high-dimensional noise, PCA 
may become sensitive to outliers within the dataset. To combat this, 
we can instead model PCA as a convex optimization problem with the cost function</p>
<div class="math">$$\min_{L,S} ||L||_* + \lambda ||S||_1$$</div>
<p>such that <span class="math">\(X = L + S\)</span>, where L is a low-rank component and S is a sparse component.
This minimizes the sum of the singular values for the matrix L and the l1-norm of the
matrix S. The sparse matrix is associated with noise.</p>
<h2>Huge Dimensionality (n &gt;&gt; 1, d &gt;&gt; 1)</h2>
<p>When you have a large number of samples with extremely high dimensions, the computational 
workhouse - SVD - becomes computationally expensive. Rather then take a full SVD, 
it has been shown that taking randomized SVD can capture the principle components of the data.</p>
<p>The randomized SVD approach essentially says that:</p>
<ul>
<li>choose a random unitary projection matrix (<span class="math">\(d \times r\)</span>)</li>
<li>multiply that with your data matrix and apply PCA</li>
<li>the principle components of that data matrix has <strong>high probability</strong> of aligning 
with the principle components of the original large data matrix</li>
</ul>
<p>Note: The important point here is that the unitary projection matrix must 
be chosen at random. Good choices are listed in <a href="https://scikit-learn.org/stable/modules/random_projection.html">sklearn</a>.</p>
<h1>Questions to Test Understanding</h1>
<ol>
<li>
<p>Given eigenvalues of the covariance matrix, what is one way to measure <em>variance explained</em> proportion for each component?</p>
<p>Take the eigenvalues and sort them, then for each eigenvalue, divide it by the trace of the covariance matrix. This 
results in a ratio between the corresponding jth eigenvalue and the total sum of all the eigenvalues, which gives 
the proportion of variance explained in the principle components.</p>
</li>
<li>
<p>By performing z-score standardization of the data matrix, the covariance matrix of the data is now equivalent to the correlation matrix?</p>
<p>By performing z-score standardization, the columns of the data matrix now all have sample variance equal to 1. Thus, the 
correlation is defined as:</p>
<p>
<div class="math">$$\rho_{X, Y} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}$$</div>
</p>
<p>which is equivalent to the covariance when the sample variances are unitary.</p>
</li>
<li></li>
</ol>
<h1>References:</h1>
<ol>
<li>Jolliffe IT, Cadima J. (2016). Principal component analysis: a review and recent developments.Phil. Trans. R. Soc. A 374:20150202. http://dx.doi.org/10.1098/rsta.2015.0202</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="/blog/2019/06/gaussian-generative-models/">Linear Gaussian Models</a></li>
        <li><a href="/blog/2019/06/optimization-landscape-overview/">Optimization: Convex, Nonlinear, Unconstrained and Constrained</a></li>
        <li><a href="/blog/2021/02/writing-journal-papers/">Writing an Academic Journal Paper</a></li>
        <li><a href="/blog/2020/12/real-analysis/">Real Analysis (Lebesgue Integration, Differentiation and Measure)</a></li>
        <li><a href="/blog/2018/06/whitaker-summary-experience/">Using The Virtual Brain (TVB) to Understand Algorithms</a></li>
    </ul>
</section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://scholar.google.com/citations?user=KxY17KcAAAAJ&hl=en"><i class="fa fa-graduation-cap-square fa-lg"></i> google scholar</a></li>
    <li class="list-group-item"><a href="https://twitter.com/adam2392"><i class="fa fa-twitter-square fa-lg"></i> twitter</a></li>
    <li class="list-group-item"><a href="https://stackexchange.com/users/4494355/ajl123"><i class="fa fa-stack-overflow fa-lg"></i> stack-overflow</a></li>
    <li class="list-group-item"><a href="https://github.com/adam2392"><i class="fa fa-github-square fa-lg"></i> github</a></li>
    <li class="list-group-item"><a href="https://www.linkedin.com/in/adam2392"><i class="fa fa-linkedin-square fa-lg"></i> linkedin</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Github -->
<li class="list-group-item">
  <h4><i class="fa fa-github fa-lg"></i><span class="icon-label">GitHub Repos</span></h4>
  <div id="gh_repos">
    <p class="list-group-item">Status updating...</p>
  </div>
</li>
<!-- End Sidebar/Github -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<!-- End Content Container -->

<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2022 Adam Li
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>



<!-- GitHub JS Code -->
<script type="text/javascript">
$(document).ready(function () {
  if (!window.jXHR) {
    var jxhr = document.createElement('script');
    jxhr.type = 'text/javascript';
    jxhr.src = '/theme/js/jXHR.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(jxhr, s);
  }

  github.showRepos({
    user: 'adam2392',
    count: 3,
    skip_forks: false,
    target: '#gh_repos'
  });
});
</script>
<script src="/theme/js/github.js" type="text/javascript"></script>
<!-- End GitHub JS Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-106551801-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->


</body>
</html>